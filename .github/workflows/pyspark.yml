# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

name: GraphAr PySpark CI

on:
  # Trigger the workflow on push or pull request,
  # but only for the main branch
  push:
    branches:
      - main
    paths:
      - 'maven-projects/spark/**'
      - 'pyspark/**'
      - '.github/workflows/pyspark.yml'
  pull_request:
    branches:
      - main
    paths:
      - 'maven-projects/spark/**'
      - 'pyspark/**'
      - '.github/workflows/pyspark.yml'

concurrency:
  group: ${{ github.repository }}-${{ github.event.number || github.head_ref || github.sha }}-${{ github.workflow }}
  cancel-in-progress: true

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
      with:
          submodules: true

    - name: Install Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install Poetry
      working-directory: pyspark
      run: |
        yes | sudo python3 -m pip install poetry --quiet
        poetry env use python3

    - name: Lint
      working-directory: pyspark
      run: |
        make install_lint
        make lint
    
    - name: Build Docs
      working-directory: pyspark
      run: |
        make install_docs
        make docs

    - name: Install Spark Scala && PySpark
      working-directory: pyspark
      run: make install_test

    - name: Run PyTest and Generate Coverage Report
      working-directory: pyspark
      run: make coverage_report

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v4
      with:
        flag: pyspark
        files: ./coverage.xml
        token: ${{ secrets.CODECOV_TOKEN }}
